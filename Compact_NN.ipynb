{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD0szh4n7s8c4t21+2qf5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/showravj2-create/Compact-NN-pipeline-/blob/main/Compact_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF8ii_9nX2ef",
        "outputId": "49235600-4403-4127-8f12-1a2471b49bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1/40] loss=2.4280 val_acc=0.2139\n",
            "[  4/40] loss=1.4846 val_acc=0.6833\n",
            "[  8/40] loss=0.9453 val_acc=0.8333\n",
            "[ 12/40] loss=0.6854 val_acc=0.8639\n",
            "[ 16/40] loss=0.5351 val_acc=0.8861\n",
            "[ 20/40] loss=0.4383 val_acc=0.9028\n",
            "[ 24/40] loss=0.3719 val_acc=0.9167\n",
            "[ 28/40] loss=0.3236 val_acc=0.9167\n",
            "[ 32/40] loss=0.2871 val_acc=0.9194\n",
            "[ 36/40] loss=0.2582 val_acc=0.9306\n",
            "[ 40/40] loss=0.2349 val_acc=0.9361\n",
            ">>> Final test accuracy: 0.9389\n",
            "Artifacts saved to: out\n",
            "\n",
            "Quick summary (copy this in an email/GitHub README):\n",
            "Model trained: test_acc=0.9389, epochs=40, hidden=128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3575058474.py:231: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"trained_at\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        }
      ],
      "source": [
         
      
        "import os\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# --- Reproducibility helpers ---\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# --- Simple feed-forward neural network implemented with NumPy ---\n",
        "class SimpleMLP:\n",
        "    \"\"\"A tiny, clear MLP implemented in NumPy (sufficient to demonstrate core ideas).\"\"\"\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, seed=0):\n",
        "        rng = np.random.RandomState(seed)\n",
        "        # Xavier init\n",
        "        self.W1 = rng.randn(in_dim, hidden_dim) * np.sqrt(2.0 / (in_dim + hidden_dim))\n",
        "        self.b1 = np.zeros((hidden_dim,))\n",
        "        self.W2 = rng.randn(hidden_dim, out_dim) * np.sqrt(2.0 / (hidden_dim + out_dim))\n",
        "        self.b2 = np.zeros((out_dim,))\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_deriv(x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        # numerically stable\n",
        "        x = x - x.max(axis=1, keepdims=True)\n",
        "        ex = np.exp(x)\n",
        "        return ex / ex.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        z1 = X.dot(self.W1) + self.b1\n",
        "        a1 = self.relu(z1)\n",
        "        logits = a1.dot(self.W2) + self.b2\n",
        "        probs = self.softmax(logits)\n",
        "        cache = {\"X\": X, \"z1\": z1, \"a1\": a1, \"logits\": logits, \"probs\": probs}\n",
        "        return probs, cache\n",
        "\n",
        "    def compute_loss(self, probs, y_onehot):\n",
        "        # cross-entropy\n",
        "        N = y_onehot.shape[0]\n",
        "        eps = 1e-12\n",
        "        # Convert sparse y_onehot to dense for element-wise multiplication\n",
        "        if hasattr(y_onehot, 'toarray'):\n",
        "            y_onehot = y_onehot.toarray()\n",
        "        loss = -np.sum(y_onehot * np.log(probs + eps)) / N\n",
        "        return loss\n",
        "\n",
        "    def backward(self, cache, y_onehot):\n",
        "        N = y_onehot.shape[0]\n",
        "        probs = cache[\"probs\"]\n",
        "        a1 = cache[\"a1\"]\n",
        "        X = cache[\"X\"]\n",
        "\n",
        "        # Convert sparse y_onehot to dense if necessary\n",
        "        if hasattr(y_onehot, 'toarray'):\n",
        "             y_onehot = y_onehot.toarray()\n",
        "\n",
        "        dlogits = (probs - y_onehot) / N  # shape: (N, C)\n",
        "        dW2 = a1.T.dot(dlogits)  # (H, C)\n",
        "        db2 = dlogits.sum(axis=0)\n",
        "        da1 = dlogits.dot(self.W2.T)  # (N, H)\n",
        "        dz1 = da1 * self.relu_deriv(cache[\"z1\"])\n",
        "        dW1 = X.T.dot(dz1)\n",
        "        db1 = dz1.sum(axis=0)\n",
        "\n",
        "        grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "        return grads\n",
        "\n",
        "    def step(self, grads, lr=1e-3):\n",
        "        self.W1 -= lr * grads[\"dW1\"]\n",
        "        self.b1 -= lr * grads[\"db1\"]\n",
        "        self.W2 -= lr * grads[\"dW2\"]\n",
        "        self.b2 -= lr * grads[\"db2\"]\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs, _ = self.forward(X)\n",
        "        return probs.argmax(axis=1)\n",
        "\n",
        "    def save(self, path):\n",
        "        np.savez_compressed(path,\n",
        "                           W1=self.W1, b1=self.b1,\n",
        "                           W2=self.W2, b2=self.b2)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        d = np.load(path)\n",
        "        inst = cls(1,1,1)  # temporary shape; will replace attributes\n",
        "        inst.W1 = d[\"W1\"]\n",
        "        inst.b1 = d[\"b1\"]\n",
        "        inst.W2 = d[\"W2\"]\n",
        "        inst.b2 = d[\"b2\"]\n",
        "        return inst\n",
        "\n",
        "# --- Small utilities for plotting and reporting ---\n",
        "def plot_training(history, out_dir):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "    ax[0].plot(history[\"train_loss\"], label=\"train\")\n",
        "    ax[0].set_title(\"Loss\")\n",
        "    ax[0].set_xlabel(\"epoch\")\n",
        "    ax[0].legend()\n",
        "    ax[1].plot(history[\"val_acc\"], label=\"val_acc\")\n",
        "    ax[1].set_title(\"Validation Accuracy\")\n",
        "    ax[1].set_xlabel(\"epoch\")\n",
        "    ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    p = os.path.join(out_dir, \"training_plots.png\")\n",
        "    plt.savefig(p)\n",
        "    plt.close(fig)\n",
        "    return p\n",
        "\n",
        "def plot_confusion(y_true, y_pred, classes, out_dir):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(6,5))\n",
        "    im = ax.imshow(cm, interpolation='nearest')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set_xticks(np.arange(len(classes)))\n",
        "    ax.set_yticks(np.arange(len(classes)))\n",
        "    ax.set_xticklabels(classes, rotation=45)\n",
        "    ax.set_yticklabels(classes)\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    p = os.path.join(out_dir, \"confusion_matrix.png\")\n",
        "    plt.savefig(p)\n",
        "    plt.close(fig)\n",
        "    return p\n",
        "\n",
        "# --- Data loading & preprocessing ---\n",
        "def load_and_preprocess(test_size=0.2, val_size=0.1, seed=0):\n",
        "    digits = load_digits()\n",
        "    X = digits.data.astype(np.float32)  # shape (n_samples, 64)\n",
        "    y = digits.target.astype(np.int64)\n",
        "    classes = [str(i) for i in range(10)]\n",
        "\n",
        "    # train/val/test split\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(test_size + val_size), random_state=seed, stratify=y\n",
        "    )\n",
        "    # separate val and test from X_temp\n",
        "    val_fraction = val_size / (test_size + val_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_fraction, random_state=seed, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    onehot = OneHotEncoder(categories='auto')\n",
        "    y_train_oh = onehot.fit_transform(y_train.reshape(-1,1))\n",
        "    y_val_oh = onehot.transform(y_val.reshape(-1,1))\n",
        "\n",
        "    return (X_train, y_train, y_train_oh,\n",
        "            X_val, y_val, y_val_oh,\n",
        "            X_test, y_test, classes)\n",
        "\n",
        "# --- Training loop ---\n",
        "def train(model, data, epochs=50, lr=0.01, batch_size=64, out_dir=\"out\"):\n",
        "    X_train, y_train, y_train_oh, X_val, y_val, y_val_oh, X_test, y_test, classes = data\n",
        "    n = X_train.shape[0]\n",
        "    history = {\"train_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Shuffle\n",
        "        idx = np.random.permutation(n)\n",
        "        X_sh, y_sh, y_sh_oh = X_train[idx], y_train[idx], y_train_oh[idx]\n",
        "\n",
        "        # Mini-batch training\n",
        "        epoch_loss = 0.0\n",
        "        for i in range(0, n, batch_size):\n",
        "            Xb = X_sh[i:i+batch_size]\n",
        "            yb_oh = y_sh_oh[i:i+batch_size]\n",
        "            probs, cache = model.forward(Xb)\n",
        "            loss = model.compute_loss(probs, yb_oh)\n",
        "            grads = model.backward(cache, yb_oh)\n",
        "            model.step(grads, lr=lr)\n",
        "            epoch_loss += loss * Xb.shape[0]\n",
        "\n",
        "        epoch_loss /= n\n",
        "        # validation\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_acc = accuracy_score(y_val, y_val_pred)\n",
        "        history[\"train_loss\"].append(epoch_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if epoch % max(1, epochs // 10) == 0 or epoch == 1:\n",
        "            print(f\"[{epoch:3d}/{epochs}] loss={epoch_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\">>> Final test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    model_path = os.path.join(out_dir, \"model.npz\")\n",
        "    model.save(model_path)\n",
        "    training_plot = plot_training(history, out_dir)\n",
        "    conf_plot = plot_confusion(y_test, y_test_pred, classes, out_dir)\n",
        "    report = classification_report(y_test, y_test_pred, digits=4)\n",
        "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\") as f:\n",
        "        f.write(report)\n",
        "\n",
        "    metadata = {\n",
        "        \"trained_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"test_accuracy\": float(test_acc),\n",
        "        \"n_train\": int(n),\n",
        "        \"model_file\": model_path,\n",
        "        \"training_plot\": training_plot,\n",
        "        \"confusion_plot\": conf_plot,\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"Artifacts saved to: {out_dir}\")\n",
        "    return history, metadata\n",
        "\n",
        "# --- CLI & Entrypoint ---\n",
        "def main():\n",
        "    # Set parameters directly in the notebook\n",
        "    epochs = 40\n",
        "    lr = 0.01\n",
        "    hidden = 128\n",
        "    batch = 64\n",
        "    seed = 0\n",
        "    out = \"out\"\n",
        "\n",
        "    set_seed(seed)\n",
        "    data = load_and_preprocess(seed=seed)\n",
        "    in_dim = data[0].shape[1]\n",
        "    out_dim = 10\n",
        "    model = SimpleMLP(in_dim=in_dim, hidden_dim=hidden, out_dim=out_dim, seed=seed)\n",
        "    history, metadata = train(model, data, epochs=epochs, lr=lr, batch_size=batch, out_dir=out)\n",
        "    # Print short summary for quick copy-paste in an application email\n",
        "    summary = f\"Model trained: test_acc={metadata['test_accuracy']:.4f}, epochs={epochs}, hidden={hidden}\"\n",
        "    print(\"\\nQuick summary (copy this in an email/GitHub README):\")\n",
        "    print(summary)\n",
        "\n",
        "main()"
      ]
    }
  ]
}
